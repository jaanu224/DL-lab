{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d7ba422",
   "metadata": {},
   "source": [
    "# Module 2 — Experiment 2 (MLP): Optimizers & Learning‑Rate Schedules on Fashion‑MNIST\n",
    "\n",
    "**Question (L3 – Apply, 2 lines):**  \n",
    "Train the same **MLP** on **Fashion‑MNIST** using **SGD, SGD+Momentum, RMSProp, and Adam**, and add an **exponential LR decay** variant. Compare convergence speed, stability, and accuracy via validation curves and test metrics.\n",
    "\n",
    "**Learning Targets:** Mini‑batch GD, Momentum, RMSProp, Adam, LR decay, Vanishing/Exploding awareness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e508f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = (x_train.astype('float32')/255.0).reshape(-1, 28*28)\n",
    "x_test  = (x_test.astype('float32')/255.0).reshape(-1, 28*28)\n",
    "print('Train:', x_train.shape, y_train.shape, '| Test:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP backbone (kept identical across optimizers for fairness)\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def make_mlp():\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def make_optimizer(name='sgd', lr=0.01, momentum=0.9):\n",
    "    if name == 'sgd':\n",
    "        return tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    if name == 'momentum':\n",
    "        return tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "    if name == 'rmsprop':\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=lr, rho=0.9)\n",
    "    if name == 'adam':\n",
    "        return tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    raise ValueError(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utility with optional LR scheduler\n",
    "def train_with_opt(opt_name, base_lr=0.01, epochs=8, batch=128, use_exp_decay=False):\n",
    "    model = make_mlp()\n",
    "    if use_exp_decay:\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=base_lr, decay_steps=200, decay_rate=0.96, staircase=True\n",
    "        )\n",
    "        optimizer = make_optimizer(opt_name, lr=lr_schedule)\n",
    "    else:\n",
    "        optimizer = make_optimizer(opt_name, lr=base_lr)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    h = model.fit(x_train, y_train, validation_split=0.2, epochs=epochs, batch_size=batch, verbose=1)\n",
    "    test_acc = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "    return h, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691900ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments (no LR decay)\n",
    "EPOCHS = 8\n",
    "hist = {}\n",
    "test_accs = {}\n",
    "for name in ['sgd', 'momentum', 'rmsprop', 'adam']:\n",
    "    h, acc = train_with_opt(name, base_lr=0.01, epochs=EPOCHS, use_exp_decay=False)\n",
    "    hist[name] = h\n",
    "    test_accs[name] = acc\n",
    "print('Test accuracies:', {k: round(v,4) for k,v in test_accs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59674ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add learning-rate decay with Adam (example)\n",
    "h_decay, acc_decay = train_with_opt('adam', base_lr=0.005, epochs=EPOCHS, use_exp_decay=True)\n",
    "hist['adam+expdecay'] = h_decay\n",
    "test_accs['adam+expdecay'] = acc_decay\n",
    "print('Adam+ExpDecay test acc:', round(acc_decay,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation accuracy and loss for all runs\n",
    "def plot_histories(histories, metric='val_accuracy', title='Validation Accuracy'):\n",
    "    fig, ax = plt.subplots()\n",
    "    for name, h in histories.items():\n",
    "        ax.plot(h.history[metric], label=name)\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel(metric.replace('_',' ').title())\n",
    "    ax.set_title(title); ax.legend(); plt.show()\n",
    "\n",
    "plot_histories(hist, 'val_accuracy', 'Validation Accuracy (MLP Optimizers & LR)')\n",
    "plot_histories(hist, 'val_loss', 'Validation Loss (MLP Optimizers & LR)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562db474",
   "metadata": {},
   "source": [
    "### Result & Inference (to be written by student)\n",
    "- Which optimizer converged fastest and which gave best accuracy?\n",
    "- Did learning‑rate decay improve stability or final performance?\n",
    "- Any signs of vanishing/exploding gradients (e.g., unstable/flat loss)?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
