{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d54ecb",
   "metadata": {},
   "source": [
    "# Module 2 — Experiment 1: Regularization & Batch Normalization (MNIST)\n",
    "\n",
    "**Question (L3 – Apply):**  \n",
    "Implement and compare the effects of **L2 regularization**, **Dropout**, and **Batch Normalization** in a deep neural network. Train on **MNIST**, and analyze how these techniques impact **overfitting**, **training stability**, and **generalization**.\n",
    "\n",
    "**Learning Targets:** Regularization, Dropout, BatchNorm, Overfitting/Generalization, Validation curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test  = x_test.astype('float32')/255.0\n",
    "\n",
    "# Add channel dimension for Conv layers\n",
    "x_train = x_train[..., None]\n",
    "x_test  = x_test[..., None]\n",
    "\n",
    "print('Train:', x_train.shape, y_train.shape, '| Test:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common utility: plot history\n",
    "def plot_histories(histories, metric='val_accuracy', title='Validation Accuracy'):\n",
    "    fig, ax = plt.subplots()\n",
    "    for name, h in histories.items():\n",
    "        ax.plot(h.history[metric], label=name)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(metric.replace('_',' ').title())\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a82ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model variants\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def make_baseline():\n",
    "    m = models.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=(28,28,1)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def make_l2(l2=1e-4):\n",
    "    m = models.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(l2), input_shape=(28,28,1)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu', kernel_regularizer=regularizers.l2(l2)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def make_dropout(p=0.5):\n",
    "    m = models.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=(28,28,1)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(p),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(p),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(p),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def make_batchnorm():\n",
    "    m = models.Sequential([\n",
    "        layers.Conv2D(32, 3, input_shape=(28,28,1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539efd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models (use small epochs here; increase for better separation)\n",
    "EPOCHS = 5\n",
    "BATCH = 128\n",
    "hist = {}\n",
    "for name, maker in [\n",
    "    ('baseline', make_baseline),\n",
    "    ('l2', make_l2),\n",
    "    ('dropout', make_dropout),\n",
    "    ('batchnorm', make_batchnorm)\n",
    "]:\n",
    "    model = maker()\n",
    "    h = model.fit(x_train, y_train, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH, verbose=1)\n",
    "    hist[name] = h\n",
    "    print(name, 'test acc:', model.evaluate(x_test, y_test, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize validation accuracy & loss\n",
    "plot_histories(hist, 'val_accuracy', 'Validation Accuracy (Regularization Variants)')\n",
    "plot_histories(hist, 'val_loss', 'Validation Loss (Regularization Variants)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e234cc",
   "metadata": {},
   "source": [
    "### Result & Inference (to be written by student)\n",
    "- Which variant overfit least and why?\n",
    "- Which variant trained fastest or most stably?\n",
    "- Summarize the impact of L2, Dropout, and BatchNorm on generalization.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
