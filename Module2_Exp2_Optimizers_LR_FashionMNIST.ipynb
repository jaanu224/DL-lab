{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf9b0cf",
   "metadata": {},
   "source": [
    "# Module 2 — Experiment 2: Optimizers & Learning Rate Schedules (Fashion‑MNIST)\n",
    "\n",
    "**Question (L3 – Apply):**  \n",
    "Apply and compare **SGD**, **SGD+Momentum**, **RMSProp**, and **Adam**, along with a **learning‑rate decay** strategy. Use **Fashion‑MNIST** and evaluate **convergence speed**, **stability**, and **accuracy**.\n",
    "\n",
    "**Learning Targets:** Mini‑batch GD, Momentum, RMSProp, Adam, LR decay, Vanishing/Exploding awareness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test  = x_test.astype('float32')/255.0\n",
    "x_train = x_train[..., None]\n",
    "x_test  = x_test[..., None]\n",
    "\n",
    "print('Train:', x_train.shape, y_train.shape, '| Test:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition (same backbone for fairness)\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def make_cnn():\n",
    "    m = models.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=(28,28,1)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return m\n",
    "\n",
    "# LR schedule example: exponential decay\n",
    "def make_optimizer(name='sgd', lr=0.01, momentum=0.9):\n",
    "    if name == 'sgd':\n",
    "        return tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    if name == 'momentum':\n",
    "        return tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "    if name == 'rmsprop':\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=lr, rho=0.9)\n",
    "    if name == 'adam':\n",
    "        return tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    raise ValueError(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd08a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utility with optional LR scheduler\n",
    "def train_with_opt(opt_name, base_lr=0.01, epochs=5, batch=128, use_exp_decay=False):\n",
    "    model = make_cnn()\n",
    "    if use_exp_decay:\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=base_lr,\n",
    "            decay_steps=200, decay_rate=0.96, staircase=True\n",
    "        )\n",
    "        optimizer = make_optimizer(opt_name, lr=lr_schedule)\n",
    "    else:\n",
    "        optimizer = make_optimizer(opt_name, lr=base_lr)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    h = model.fit(x_train, y_train, validation_split=0.2, epochs=epochs, batch_size=batch, verbose=1)\n",
    "    test_acc = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "    return h, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "EPOCHS = 6\n",
    "hist = {}\n",
    "test_accs = {}\n",
    "for name in ['sgd', 'momentum', 'rmsprop', 'adam']:\n",
    "    h, acc = train_with_opt(name, base_lr=0.01, epochs=EPOCHS, use_exp_decay=False)\n",
    "    hist[name] = h\n",
    "    test_accs[name] = acc\n",
    "print('Test accuracies:', {k: round(v,4) for k,v in test_accs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add learning-rate decay with Adam (as an example)\n",
    "h_decay, acc_decay = train_with_opt('adam', base_lr=0.005, epochs=EPOCHS, use_exp_decay=True)\n",
    "hist['adam+expdecay'] = h_decay\n",
    "test_accs['adam+expdecay'] = acc_decay\n",
    "print('Adam+ExpDecay test acc:', round(acc_decay,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74563074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation accuracy and loss for all runs\n",
    "def plot_histories(histories, metric='val_accuracy', title='Validation Accuracy'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots()\n",
    "    for name, h in histories.items():\n",
    "        ax.plot(h.history[metric], label=name)\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel(metric.replace('_',' ').title())\n",
    "    ax.set_title(title); ax.legend(); plt.show()\n",
    "\n",
    "plot_histories(hist, 'val_accuracy', 'Validation Accuracy (Optimizers & LR)')\n",
    "plot_histories(hist, 'val_loss', 'Validation Loss (Optimizers & LR)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe827e",
   "metadata": {},
   "source": [
    "### Result & Inference (to be written by student)\n",
    "- Which optimizer converged fastest and which gave best accuracy?\n",
    "- Did learning‑rate decay improve stability or final performance?\n",
    "- Any signs of vanishing/exploding gradients (e.g., unstable/flat loss)?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
