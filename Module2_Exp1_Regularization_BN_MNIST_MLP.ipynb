{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88cc8b1e",
   "metadata": {},
   "source": [
    "# Module 2 — Experiment 1 (MLP): Regularization & Batch Normalization on MNIST\n",
    "\n",
    "**Question (L3 – Apply, 2 lines):**  \n",
    "Train a simple **MLP** on **MNIST** and compare **L2 regularization, Dropout, and Batch Normalization** against a baseline. Analyze overfitting, training stability, and generalization using validation curves and test accuracy.\n",
    "\n",
    "**Learning Targets:** Regularization, Dropout, BatchNorm, Overfitting/Generalization, Validation curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da489b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = (x_train.astype('float32')/255.0).reshape(-1, 28*28)\n",
    "x_test  = (x_test.astype('float32')/255.0).reshape(-1, 28*28)\n",
    "\n",
    "print('Train:', x_train.shape, y_train.shape, '| Test:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to plot histories\n",
    "def plot_histories(histories, metric='val_accuracy', title='Validation Accuracy'):\n",
    "    fig, ax = plt.subplots()\n",
    "    for name, h in histories.items():\n",
    "        ax.plot(h.history[metric], label=name)\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel(metric.replace('_',' ').title())\n",
    "    ax.set_title(title); ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01091ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP variants\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def make_baseline():\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def make_l2(l2=1e-4):\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def make_dropout(p=0.5):\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(p),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(p),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def make_batchnorm():\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(256, use_bias=False), layers.BatchNormalization(), layers.Activation('relu'),\n",
    "        layers.Dense(128, use_bias=False), layers.BatchNormalization(), layers.Activation('relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models (increase EPOCHS for deeper analysis)\n",
    "EPOCHS = 8\n",
    "BATCH = 128\n",
    "hist = {}\n",
    "test_acc = {}\n",
    "for name, maker in [('baseline', make_baseline),\n",
    "                    ('l2', make_l2),\n",
    "                    ('dropout', make_dropout),\n",
    "                    ('batchnorm', make_batchnorm)]:\n",
    "    model = maker()\n",
    "    h = model.fit(x_train, y_train, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH, verbose=1)\n",
    "    hist[name] = h\n",
    "    test_acc[name] = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "    print(name, 'test acc:', round(test_acc[name], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efdf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize validation accuracy & loss\n",
    "plot_histories(hist, 'val_accuracy', 'Validation Accuracy (MLP Regularization Variants)')\n",
    "plot_histories(hist, 'val_loss', 'Validation Loss (MLP Regularization Variants)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a120d",
   "metadata": {},
   "source": [
    "### Result & Inference (to be written by student)\n",
    "- Which variant overfit least and why?\n",
    "- Which variant improved stability or convergence?\n",
    "- Summarize the impact of L2, Dropout, and BatchNorm on generalization.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
