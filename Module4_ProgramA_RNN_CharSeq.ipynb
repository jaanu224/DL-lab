{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49025e2d",
   "metadata": {},
   "source": [
    "# Module 4 — Program A: Basic Sequence Modelling with RNN (Character-level)\n",
    "\n",
    "**Covers:** Sequence modelling, SimpleRNN, backpropagation through time, sequence generation, vanishing gradients (short vs long sequences).\n",
    "\n",
    "**Question (Apply – L3):** Implement a character-level RNN to learn next-character prediction on a small corpus and generate text.\n",
    "\n",
    "**Question (Analyze – L4):** Analyze how sequence length affects prediction quality and discuss vanishing gradients observed in training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Toy corpus (public‑domain snippets). Feel free to replace with your own text.\n",
    "corpus = (\n",
    "    \"deep learning enables computers to learn from data. \"\n",
    "    \"recurrent neural networks process sequences step by step. \"\n",
    "    \"lstm and gru mitigate vanishing gradients in long sequences. \"\n",
    "    \"attention and transformers capture long range dependencies.\"\n",
    ")\n",
    "chars = sorted(list(set(corpus)))\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (X, y) for next-char prediction\n",
    "def make_xy(text, seq_len=40, step=3):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(text)-seq_len-1, step):\n",
    "        seq = text[i:i+seq_len]\n",
    "        nxt = text[i+seq_len]\n",
    "        X.append([stoi[c] for c in seq])\n",
    "        y.append(stoi[nxt])\n",
    "    X = np.array(X, dtype=np.int32)\n",
    "    y = np.array(y, dtype=np.int32)\n",
    "    return X, y\n",
    "\n",
    "SEQ_LEN = 40\n",
    "X, y = make_xy(corpus, seq_len=SEQ_LEN, step=1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d74db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode inputs for SimpleRNN (or use Embedding + RNN). We'll use Embedding for efficiency.\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "embed_dim = 32\n",
    "rnn_units = 64\n",
    "\n",
    "inputs = layers.Input(shape=(SEQ_LEN,), dtype='int32')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "x = layers.SimpleRNN(rnn_units, return_sequences=False)(x)\n",
    "outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "n = len(X)\n",
    "idx = np.arange(n)\n",
    "np.random.shuffle(idx)\n",
    "tr = int(0.85*n)\n",
    "X_train, y_train = X[idx[:tr]], y[idx[:tr]]\n",
    "X_val, y_val = X[idx[tr:]], y[idx[tr:]]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=25, batch_size=64, verbose=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hist.history['loss'], label='train')\n",
    "ax.plot(hist.history['val_loss'], label='val')\n",
    "ax.set_title('Training vs Validation Loss')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.legend(); plt.show()\n",
    "\n",
    "print('Final val accuracy:', round(hist.history['val_accuracy'][-1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation helper\n",
    "def sample(probs, temperature=1.0):\n",
    "    probs = np.asarray(probs).astype('float64')\n",
    "    if temperature != 1.0:\n",
    "        probs = np.log(probs + 1e-9) / temperature\n",
    "        probs = np.exp(probs)\n",
    "        probs = probs / np.sum(probs)\n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def generate_text(prefix, length=200, temperature=0.8):\n",
    "    s = prefix\n",
    "    for _ in range(length):\n",
    "        # ensure length SEQ_LEN input\n",
    "        seed = s[-SEQ_LEN:]\n",
    "        seed_idx = np.array([[stoi.get(c, 0) for c in seed.ljust(SEQ_LEN)[:SEQ_LEN]]])\n",
    "        p = model.predict(seed_idx, verbose=0)[0]\n",
    "        idx = sample(p, temperature)\n",
    "        s += itos[idx]\n",
    "    return s\n",
    "\n",
    "print(generate_text('deep learning ', length=200, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac4e13",
   "metadata": {},
   "source": [
    "### Analyze (L4)\n",
    "- Re-run with **SEQ_LEN = 20 vs 60** and compare loss/accuracy and generated text coherence.\n",
    "- Observe if longer sequences make optimization harder (hint: vanishing gradients), and justify your findings.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
